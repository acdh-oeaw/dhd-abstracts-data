<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" xml:id="STR_BEL_Phillip_Benjamin_Re_Experiencing_History__A_Platform">
    <teiHeader>
        <fileDesc>
            <titleStmt>
                <title type="full">
                    <title type="main">Re-Experiencing History: A Platform for the Re-Enactment of Historical Events with Multimodal Large Language Models</title>
                    <title type="sub"/>
                </title>
                <author>
                    <persName>
                        <surname>Ströbel</surname>
                        <forename>Phillip Benjamin</forename>
                    </persName>
                    <affiliation>University of Zurich, Schweiz</affiliation>
                    <email>phillip.stroebel@uzh.ch</email>
                    <idno type="ORCID">0000-0003-2063-5495</idno>
                </author>
                <author>
                    <persName>
                        <surname>Maier</surname>
                        <forename>Felix Klaus</forename>
                    </persName>
                    <affiliation>University of Zurich, Schweiz</affiliation>
                    <email>felix.maier@hist.uzh.ch</email>
                    <idno type="ORCID">0000-0002-5578-723X</idno>
                </author>
            </titleStmt>
            <editionStmt>
                <edition>
                    <date>2024-07-22T14:05:36.765760820</date>
                </edition>
            </editionStmt>
            <publicationStmt>
                <publisher>Bielefeld Computational Literary Studies Group</publisher>
                <address>
                    <addrLine>Universität Bielefeld</addrLine>
                    <addrLine>Universitätsstraße 25</addrLine>
                    <addrLine>33615 Bielefeld</addrLine>
                    <addrLine>Deutschland</addrLine>
                </address>
                <publisher>Digital History</publisher>
                <address>
                    <addrLine>Universität Bielefeld</addrLine>
                    <addrLine>Universitätsstraße 25</addrLine>
                    <addrLine>33615 Bielefeld</addrLine>
                    <addrLine>Deutschland</addrLine>
                </address>
                <publisher>Digital Linguistics Lab</publisher>
                <address>
                    <addrLine>Universität Bielefeld</addrLine>
                    <addrLine>Universitätsstraße 25</addrLine>
                    <addrLine>33615 Bielefeld</addrLine>
                    <addrLine>Deutschland</addrLine>
                </address>
            </publicationStmt>
            <sourceDesc>
                <p>Converted from an OASIS Open Document</p>
            </sourceDesc>
        </fileDesc>
        <encodingDesc>
            <appInfo>
                <application ident="DHCONVALIDATOR" version="1.22">
                    <label>DHConvalidator</label>
                </application>
            </appInfo>
        </encodingDesc>
        <profileDesc>
            <textClass>
                <keywords scheme="ConfTool" n="category">
                    <term>Paper</term>
                </keywords>
                <keywords scheme="ConfTool" n="subcategory">
                    <term>Poster</term>
                </keywords>
                <keywords scheme="ConfTool" n="keywords">
                    <term>Digital Humanities</term>
                    <term>image generation</term>
                    <term>human evaluation</term>
                    <term>automatic evaluation</term>
                    <term>history</term>
                    <term>image dataset</term>
                    <term>web application</term>
                    <term>re-enactment</term>
                </keywords>
                <keywords scheme="ConfTool" n="topics">
                    <term>Bilderfassung</term>
                    <term>Gestaltung</term>
                    <term>Bewertung</term>
                    <term>Visualisierung</term>
                    <term>Bilder</term>
                    <term>Visualisierung</term>
                </keywords>
            </textClass>
        </profileDesc>
    </teiHeader>
    <text>
        <body>
            <div type="div1" rend="DH-Heading1">
                <head>Project Description</head>
                <p>In 1982, seven people in the Chicago area tragically died after consuming cyanide-laced capsules (cf. Bergmann (2000)). The source of this tampering was initially unknown, leading to nationwide panic. Investigators were stumped until they conducted 
                    <hi rend="bold">re-enactments</hi> of the purchase and consumption of the contaminated capsules. These re-enactments enabled them to trace the tainted products to specific store shelves and identify the exact locations where the tampering occurred.
                </p>
                <p>Like criminologists, historians have to reconstruct past moments or situations. However, unlike criminologists, historians often cannot re-enact<ref n="1" target="ftn1"/> events due to the fact that their subject of investigation cannot be easily reproduced. The advent of AI in the form of powerful multimodal large language models (LLMs) is a game changer here: Historians can now prompt image generation models (mostly based on 
                    <hi rend="italic">Stable Diffusion</hi> (Rombach et al., 2022)) to re-create scenes from primary sources and research findings. This enables them to quickly visualise past events, thereby enhancing their understanding of historical moments. The project 
                    <hi rend="italic">Re-Experiencing History</hi> aims to create a platform exploiting LLMs to support users in re-enacting such historical events.
                </p>
                <p>Our research includes assessing existing multimodal LLMs regarding historical accuracy and prompt-to-image alignment (Xu et al., 2023), manipulating generated images with further prompts, and fine-tuning LLMs to improve historical accuracy. The 
                    <hi rend="bold">interdisciplinary approach</hi> where computational linguists work with historians is crucial, as Hutson, Huffman, and Ratican (2024) highlighted in their work on resurrecting Mary Sibley (1800-1878) using her diaries.
                </p>
                <p>In a prototype setting, we focus on two scenarios from antiquity: the Roman triumph and the 
                    <hi rend="italic">Lupercalia</hi> festival. We assess image generation capabilities, initially focusing on DALL-E 3. Based on literature (15 sources on the triumph, 5 on the 
                    <hi rend="italic">Lupercalia</hi>), we crafted 100 prompts to generate six images per prompt, creating 600 images total.<ref n="2" target="ftn2"/> These images and prompts form a basis for further research. Figure 1 displays two images generated with the same prompt.
                </p>
                <p>
                    <figure>
                        <graphic url="Pictures/7694ba765e70088f79c32bd4aab3d167.png"/>
                        <head>Example of two images generated with the same prompt depicting a triumphal procession in ancient Rome as described in Algül (2018).</head>
                    </figure>We know these images are challenging to evaluate and that automatic evaluations correlate poorly with human judgment (Otani et al., 2023). A first human assessment of the 600 images by 20 history students, coupled with an automatic evaluation using LLMs like GPT-4o, Gemini, or Claude Sonnet 3.5 as evaluators, shows that prompt-to-image alignment on Likert scale from “1 – The image does not match the prompt at all.” to “5 – The image completely matches the prompt.” is sometimes highly overestimated and too generous (especially by Claude) (see Figure 2).<ref n="3" target="ftn3"/> Moreover, we found that the scores on the Likert scale as assessed by LLMs are usually higher for the Roman triumph than for the 
                    <hi rend="italic">Lupercalia</hi>, suggesting potential knowledge gaps. We aim to close this gap by fine-tuning language and vision models on relevant material, increasing historical accuracy.
                </p>
                <p>
                    <figure>
                        <graphic url="Pictures/b5b842cd73f71f7a37bd9e957f089fc6.png"/>
                        <head>Aggregation and comparison of scores of human ratings vs. LLM ratings on a Likert scale.</head>
                    </figure>Our application targets history students at all levels, the general public, and museums. We envision a platform that creates accurate images and allows their editing and posting in an online gallery. The application aims to foster 
                    <hi rend="bold">engagement</hi> with historical content and serve as a research tool for historians to reconstruct past events, enhancing our understanding of history (Rosenzweig and Thelen, 1998).
                </p>
                <p>This project contributes to the field of digital humanities and explores the potential of AI in historical research and education. By bridging the gap between past events and modern technology, we aim to create a more immersive and accessible approach to studying history, potentially redefining how we interact with and understand our collective past. </p>
            </div>
        </body>
        <back>
<div type="notes">
<note xml:id="ftn1" rend="footnote text" n="1"> Although re-enactment is present as a concept in, e.g., R. G. Collingwood's 
                        <hi rend="italic">Idea of History</hi> (cf. Dray (1995)), it is rather perceived as thought experiments.
                    </note>
<note xml:id="ftn2" rend="footnote text" n="2"> We will make the prompts and the generated images available soon.</note>
<note xml:id="ftn3" rend="footnote text" n="3"> We must highlight at this point that the results are preliminary and that out of the 600 images, only 544 have been rated. The evaluation is still on-going, which is why we cannot provide an inter-annotator agreement yet. However, Figure 2 shows initial tendencies of the prompt-to-image alignment when evaluated with LLMs.</note></div>
            <div type="bibliogr">
                <listBibl>
                    <head>Bibliographie</head>
                    <bibl>
                        <hi rend="bold">Algül, Aydın. </hi>2018. 
                        <hi rend="italic">The Roman Triumph: Participation, Historiography and Remembrance</hi>. 
                        <ptr target="https://www.academia.edu/43295099/The_Roman_Triumph_Participation_Historiography_and_Remembrance"/>(accessed 21 July 2024).
                    </bibl>
                    <bibl>
                        <hi rend="bold">Bergmann, J</hi>
                        <hi rend="bold">oy</hi>
                        <hi rend="bold">.</hi> 2000. A Bitter Pill. In 
                        <hi rend="italic">Chicago Reader</hi>, 
                        <ptr target="https://chicagoreader.com/news-politics/a-bitter-pill"/>(accessed 21 July 2024).
                    </bibl>
                    <bibl>
                        <hi rend="bold">Dray, W</hi>
                        <hi rend="bold">illiam</hi>
                        <hi rend="bold"> H</hi>
                        <hi rend="bold">erbert</hi>
                        <hi rend="bold">.</hi> 1995. 
                        <hi rend="italic">History as Re-Enactment: R. G. Collingwood's Idea of History</hi>. Oxford: Oxford University Press.
                    </bibl>
                    <bibl>
                        <hi rend="bold">Hutson, </hi>
                        <hi rend="bold">James</hi>
                        <hi rend="bold">, </hi>
                        <hi rend="bold">Paul </hi>
                        <hi rend="bold">Huffman and </hi>
                        <hi rend="bold">Jeremiah </hi>
                        <hi rend="bold">Ratican. </hi>2024. Digital Resurrection of Historical Figures: A Case Study on Mary Sibley through Customized ChatGPT. In 
                        <hi rend="italic">Faculty Scholarship, </hi>590, 
                        <ptr target="https://digitalcommons.lindenwood.edu/faculty-research-papers/590"/>(accessed 21 July 2024).
                    </bibl>
                    <bibl>
                        <hi rend="bold">Otani, M</hi>
                        <hi rend="bold">ayu</hi>
                        <hi rend="bold">, </hi>
                        <hi rend="bold">Riku</hi>
                        <hi rend="bold"> Togashi, </hi>
                        <hi rend="bold">Yu</hi>
                        <hi rend="bold"> Sawai, </hi>
                        <hi rend="bold">Ryosuke</hi>
                        <hi rend="bold"> Ishigami, </hi>
                        <hi rend="bold">Yute</hi>
                        <hi rend="bold"> Nakashima, </hi>
                        <hi rend="bold">Esa</hi>
                        <hi rend="bold"> Rahtu, </hi>
                        <hi rend="bold">Janne </hi>
                        <hi rend="bold">Heikkilä and </hi>
                        <hi rend="bold">Shin’ichi</hi>
                        <hi rend="bold"> Satoh.</hi> 2023. Toward Verifiable and Reproducible Human Evaluation for Text-to-Image Generation. In 
                        <hi rend="italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, </hi>14277–14286, 
                        <ptr target="https://cvpr2023.thecvf.com/virtual/2023/poster/22014"/>(accessed 21 July 2024).
                    </bibl>
                    <bibl>
                        <hi rend="bold">Rombach, R</hi>
                        <hi rend="bold">obin, Andreas</hi>
                        <hi rend="bold"> Blattmann, </hi>
                        <hi rend="bold">Dominik</hi>
                        <hi rend="bold"> Lorenz, </hi>
                        <hi rend="bold">Patrick</hi>
                        <hi rend="bold"> Esser and </hi>
                        <hi rend="bold">Björn</hi>
                        <hi rend="bold"> Ommer.</hi> 2022. High-Resolution Image Synthesis with Latent Diffusion Models. In 
                        <hi rend="italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</hi>, 10684-10695, 
                        <ptr target="https://openaccess.thecvf.com/content/CVPR2022/papers/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.pdf"/>(accessed 21 July 2024).
                    </bibl>
                    <bibl>
                        <hi rend="bold">Rosenzweig, R</hi>
                        <hi rend="bold">oy</hi>
                        <hi rend="bold"> and </hi>
                        <hi rend="bold">David</hi>
                        <hi rend="bold"> Thelen.</hi> 1998. 
                        <hi rend="italic">The Presence of the Past: Popular Uses of History in American Life</hi>. New York: Columbia University Press.
                    </bibl>
                    <bibl>
                        <hi rend="bold">Xu, J</hi>
                        <hi rend="bold">iazheng</hi>
                        <hi rend="bold">, </hi>
                        <hi rend="bold">Xiao</hi>
                        <hi rend="bold"> Liu, </hi>
                        <hi rend="bold">Yuchen</hi>
                        <hi rend="bold"> Wu, </hi>
                        <hi rend="bold">Yuxuan</hi>
                        <hi rend="bold"> Tong, </hi>
                        <hi rend="bold">Qinkai</hi>
                        <hi rend="bold"> Li, </hi>
                        <hi rend="bold">Ming</hi>
                        <hi rend="bold"> Ding, </hi>
                        <hi rend="bold">Jie</hi>
                        <hi rend="bold"> Tang and </hi>
                        <hi rend="bold">Yuxiao </hi>
                        <hi rend="bold">Dong.</hi> 2023. ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, &amp; S. Levine (eds.),
                        <hi rend="italic">Advances in Neural Information Processing Systems</hi>, 36, 15903–15935, 
                        <ptr target="https://arxiv.org/abs/2304.05977"/> (accessed 21 July 2024).
                    </bibl>
                </listBibl>
            </div>
        </back>
    </text>
</TEI>
