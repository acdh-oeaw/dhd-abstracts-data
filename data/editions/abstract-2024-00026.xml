<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:id="STALTER_Julian_ReflectAI__Reflexionsbasierte_k_nstliche_Inte" xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc>
            <titleStmt>
                <title>ReflectAI: Reflexionsbasierte künstliche Intelligenz in der Kunstgeschichte</title>
                <author>
                    <persName>
                        <surname>Stalter</surname>
                        <forename>Julian</forename>
                    </persName>
                    <affiliation>Ludwig-Maximilians-Universität München, Deutschland</affiliation>
                    <email>julian.stalter@kunstgeschichte.uni-muenchen.de</email>
                    <idno type="ORCID">0000-0003-1149-1688</idno>
                </author>
                <author>
                    <persName>
                        <surname>Springstein</surname>
                        <forename>Matthias</forename>
                    </persName>
                    <affiliation>L3S Research Center, Leibniz Universität Hannover, Deutschland; TIB – Leibniz-Informationszentrum Technik und Naturwissenschaften, Deutschland</affiliation>
                    <email>Matthias.Springstein@tib.eu</email>
                    <idno type="ORCID">0000-0002-6509-8534</idno>
                </author>
                <author>
                    <persName>
                        <surname>Kristen</surname>
                        <forename>Maximilian</forename>
                    </persName>
                    <affiliation>Ludwig-Maximilians-Universität München, Deutschland</affiliation>
                    <email>max.kristen@campus.lmu.de</email>
                </author>
                <author>
                    <persName>
                        <surname>Schneider</surname>
                        <forename>Stefanie</forename>
                    </persName>
                    <affiliation>Ludwig-Maximilians-Universität München, Deutschland</affiliation>
                    <email>stefanie.schneider@itg.uni-muenchen.de</email>
                    <idno type="ORCID">0000-0003-4915-6949</idno>
                </author>
                <author>
                    <persName>
                        <surname>Müller-Budack</surname>
                        <forename>Eric</forename>
                    </persName>
                    <affiliation>L3S Research Center, Leibniz Universität Hannover, Deutschland; TIB – Leibniz-Informationszentrum Technik und Naturwissenschaften, Deutschland</affiliation>
                    <email>Eric.Mueller@tib.eu</email>
                    <idno type="ORCID">0000-0002-6802-1241</idno>
                </author>
                <author>
                    <persName>
                        <surname>Ewerth</surname>
                        <forename>Ralph</forename>
                    </persName>
                    <affiliation>L3S Research Center, Leibniz Universität Hannover, Deutschland; TIB – Leibniz-Informationszentrum Technik und Naturwissenschaften, Deutschland</affiliation>
                    <email>Ralph.Ewerth@tib.eu</email>
                    <idno type="ORCID">0000-0003-0918-6297</idno>
                </author>
                <author>
                    <persName>
                        <surname>Kohle</surname>
                        <forename>Hubertus</forename>
                    </persName>
                    <affiliation>Ludwig-Maximilians-Universität München, Deutschland</affiliation>
                    <email>hubertus.kohle@lmu.de</email>
                    <idno type="ORCID">0000-0003-3162-1304</idno>
                </author>
            </titleStmt>
            <editionStmt>
                <edition>
                    <date>2023-12-05T09:43:00Z</date>
                </edition>
            </editionStmt>
            <publicationStmt>
                <publisher>Digital Humanities Passau</publisher>
                <address>
                    <addrLine>Universität Passau</addrLine>
                    <addrLine>Innstraße 41</addrLine>
                    <addrLine>D-94032 Passau</addrLine>
                    <addrLine>Deutschland</addrLine>
                </address>
            </publicationStmt>
            <sourceDesc>
                <p>Converted from a Word document</p>
            </sourceDesc>
        </fileDesc>
        <encodingDesc>
            <appInfo>
                <application ident="DHCONVALIDATOR" version="1.22">
                    <label>DHConvalidator</label>
                </application>
            </appInfo>
        </encodingDesc>
        <profileDesc>
            <textClass>
                <keywords scheme="ConfTool" n="category">
                    <term>Paper</term>
                </keywords>
                <keywords scheme="ConfTool" n="subcategory">
                    <term>Posterpräsentation</term>
                </keywords>
                <keywords scheme="ConfTool" n="keywords">
                    <term>Wissensgraph</term>
                    <term>multimodales Retrieval</term>
                    <term>Explainable AI</term>
                    <term>Visualisierung</term>
                    <term>Bias</term>
                </keywords>
                <keywords scheme="ConfTool" n="topics">
                    <term>Entdeckung</term>
                    <term>Bilderfassung</term>
                    <term>Annotieren</term>
                    <term>Visualisierung</term>
                    <term>Bilder</term>
                </keywords>
            </textClass>
        </profileDesc>
    </teiHeader>
    <text>
        <body>
            <p style="text-align: justify;">
                <anchor xml:id="OLE_LINK1"/>
                <anchor xml:id="OLE_LINK2"/>
                <hi style="font-size:11pt" xml:space="preserve">In der Kunstgeschichte sind Ähnlichkeitsbewertungen von Bildern von großer Bedeutung: Wölfflin analysierte Kunstwerke nach begrifflichen Gegensatzpaaren mit Doppelprojektionen, Warburg beim </hi>
                <hi rend="italic" style="font-size:11pt">Vergleichenden Sehen</hi>
                <hi style="font-size:11pt" xml:space="preserve"> nach sogenannten </hi>
                <hi rend="italic" style="font-size:11pt" xml:space="preserve">Pathosformeln </hi>
                <hi style="font-size:11pt">(Wölfflin, 1915; Hensel, 2010). Mit Verfahren aus dem Bereich des maschinellen Sehens (</hi><hi rend="italic" style="font-size:11pt">Computer Vision</hi><hi style="font-size:11pt" xml:space="preserve">) lassen sich derartige Bewertungen heute automatisieren. </hi>
                <hi rend="italic" style="font-size:11pt">State-of-the-Art</hi>
                <hi style="font-size:11pt" xml:space="preserve">-Modelle wie </hi>
                <hi rend="italic" style="font-size:11pt" xml:space="preserve">GPT </hi>
                <hi style="font-size:11pt">(</hi><hi rend="italic" style="font-size:11pt">Generative Pre-trained Transformer</hi><hi style="font-size:11pt" xml:space="preserve">; OpenAI, 2023) oder </hi>
                <hi rend="italic" style="font-size:11pt" xml:space="preserve">CLIP </hi>
                <hi style="font-size:11pt">(</hi><hi rend="italic" style="font-size:11pt">Contrastive Language-image Pre-training</hi><hi style="font-size:11pt">; Radford et al., 2021) können zudem aufgrund erweiterter Rechenkapazitäten effizienter generalisieren und damit auf nicht realweltliche Bilddaten angewandt werden. Eingesetzt wurden solche Ansätze bereits in kunsthistorischen Projekten, insbesondere in der Ähnlichkeits-basierten Bildsuche und -clusterung (Schneider et al., 2022; Offert und Bell, 2023).</hi>
            </p>
            <p style="text-align: justify;">
                <lb/>
                <hi style="font-size:11pt" xml:space="preserve">Der Einsatz von </hi>
                <hi rend="italic" style="font-size:11pt">Künstlicher Intelligenz</hi>
                <hi style="font-size:11pt" xml:space="preserve"> (</hi><hi rend="italic" style="font-size:11pt">KI</hi><hi style="font-size:11pt" xml:space="preserve">) in der kunsthistorischen Forschung eröffnet unbestreitbar neue explorative Potenziale für die Analyse von Bildähnlichkeiten. Aus methodenkritischer Perspektive ist er jedoch zu hinterfragen; insbesondere der „Black Box“-Charakter künstlicher neuronaler Netze wird diskutiert (Crawford und Paglen, 2021). Diesen Problemen soll sich das hier vorgestellte Projekt </hi>
                <hi rend="italic" style="font-size:11pt">ReflectAI</hi>
                <hi style="font-size:11pt" xml:space="preserve"> speziell für den Bereich der Kunstgeschichte annehmen. An dem Vorhaben, das im Rahmen des DFG-Schwerpunktprogramms „Das digitale Bild“ von 2022 bis 2025 gefördert wird, sind Forschende aus der Kunstgeschichte und Informatik der Universitäten München und Hannover beteiligt. Eine schematische Darstellung aller Teilmodule innerhalb des Projekts ist in Abb. 1 dargestellt.</hi>
            </p>
            <figure>
                <graphic n="1001" width="16.002cm" height="10.893777777777778cm" url="Pictures/40ad12cacdff048223f5c0ae703795f8.png" rend="inline"/>
                <head>Abb. 1: Schematische Darstellung der Erstellung kunsthistorischer Wissensgraphen auf der Basis domänenspezifischer Textkorpora, des Trainings multimodaler Modelle sowie der Visualisierungentypen, die im Rahmen von <hi rend="italic">ReflectAI</hi> entwickelt werden.</head>
            </figure>
            <div type="div1" rend="DH-Heading1">
                <head>Herausforderung „Black Box“</head>
                <p style="text-align: left; text-align: justify;">Obwohl die Ein- und Ausgabedaten neuronaler Netze bekannt sind, bleiben die internen Verarbeitungsprozesse weitgehend undurchsichtig. Ziel unseres Projekts ist es, ein tieferes Verständnis der Prozesse zu erlangen, die zu den Suchergebnissen führen. Dabei konzentrieren wir uns auf Prozesse, die zur Identifikation von Bildähnlichkeiten, zum Clustering von Bildern und zur Klassifikation anhand spezifischer Bilddetails führen. Im Sinne einer „Explainable AI“ werden Methoden untersucht, die einen Blick in diese „Black Box“ erlauben (Guidotti et al. 2019; Offert und Bell, 2021). Unsere Absicht ist, Verfahren der automatisierten Bildanalyse sowohl aus bildwissenschaftlicher als auch aus wissenschaftshistorischer Perspektive zu analysieren und in eine für kunsthistorische Forschungsprojekte optimierte Anwendung zu überführen. Mit Motivation entwickeln wir im Projekt reflexive Werkzeuge und stellen sie bereit.</p>
            </div>
            <div type="div1" rend="DH-Heading1">
                <head>Reflexive Komponenten</head>
                <div type="div2" rend="DH-Heading2">
                    <head>Expertenwissen</head>
                    <p style="text-align: left; text-align: justify;">Die Zusammensetzung der Trainingsdaten, auf deren Basis neuronale Netze und Modelle trainiert werden, ist den Anwendern oft nicht bekannt und im Sinne einer Methodenkritik schwer zugänglich. Unser Projekt beinhaltet die Optimierung dieser Modelle mit kunsthistorischen Texten und multimodalen Informationen (wie Text-Bild-Paaren), um domänenspezifisches Wissen zu integrieren. Dabei wollen wir z.B. auf Visual Language Models wie BLIP-2 (Li, 2023) oder LLAVA (Liu, 2023) zurückgreifen, wobei das Expertenwissen als Trainingsmaterial für Modelle dienen kann oder auch als zusätzlicher Input des Language Models genutzt werden kann. Außerdem trainieren wir Modelle mit Textkorpora: So können beispielsweise die Ergebnisse von Modellen, in die Kunstkritiken des 19. Jahrhunderts eingespeist wurden, mit denen verglichen werden, die auf Texten des 21. Jahrhunderts basieren. Im Rahmen des Projekts wird dazu ein multimodales Datenkorpus aus Bild- und Textquellen aufgebaut. Es umfasst derzeit 60.000 Objektbeschreibungen aus Museen, Sammlungen und Auktionshäusern, 10.000 Lexikonartikel und 50.000 wissenschaftliche Publikationen. </p>
                </div>
                <div type="div2" rend="DH-Heading2">
                    <head>Wissensgraphen</head>
                    <p style="text-align: left; text-align: justify;">Das so gesammelte Expertenwissen soll nicht nur zum Training der Modelle, sondern auch zur Erstellung von 
                        <hi rend="italic" xml:space="preserve">Wissensgraphen </hi>(<hi rend="italic">Knowledge Graphs</hi>) verwendet werden. Obwohl es Versuche gibt, kunsthistorisches Wissen aus Wissensgraphen für das Training neuronaler Netze zu nutzen, beschränken sich diese häufig auf einzelne kunsthistorische Attribute (Garcia und Vogiatzis, 2018; Schrade und Söhn, 2022). Neben den Wissensgraphen werden Szenegraphen generiert, die mit zusätzlichen Visualisierungen ein besseres Verständnis der Bildauswahl anhand der in den Suchergebnissen dargestellten Objekte und Beziehungen ermöglichen und auf Verzerrungen (<hi style="font-size:9pt">„</hi>Biases<hi style="font-size:9pt">“</hi>) oder Ähnlichkeiten hinweisen können (Suhail, 2021). Diese Szenegraphen können Beziehungen zwischen Objekten innerhalb eines Kunstwerks oder im Kontext darstellen. In Kombination mit Wissensgraphen ist eine kontrastive Analyse möglich. 
                    </p>
                </div>
                <div type="div2" rend="DH-Heading2">
                    <head>Visualisierung von Bias</head>
                    <p style="text-align: left; text-align: justify;">Unter Bias versteht man die Verzerrung der Ergebnisse aufgrund falscher Annahmen über die Trainingsdaten oder die Modelle. Dieser Bias kann historisch bedingt sein oder durch fehlerhafte oder diskriminierende Annotationen in den Trainingsdaten entstehen (Pasquinelli und Joler, 2021). Um solche Verzerrungen sichtbar zu machen, werden den Nutzenden im Rahmen des Projekts verschiedene Werkzeuge zur Verfügung gestellt, mit denen Biases visualisiert werden können; ein Beispiel ist in Abb. 2 dargestellt.</p>
                    <figure>
                        <graphic n="1002" width="9.454444444444444cm" height="8.325555555555555cm" url="Pictures/c42379cdf0c5ed04f60c9f3f251aec6f.png" rend="inline"/>
                        <head>Abb. 2: Illustration der Vorhersage von Modellen zur Erkennung von herrschenden Personen. In verzerrten Modellen werden geschlechtsspezifische Merkmale wie der Bart oder das lange Haar zur Vorhersage verwendet, in unverzerrten Modellen eher Krone und Reichsapfel.</head>
                    </figure>
                </div>
            </div>
            <div type="div1" rend="DH-Heading1">
                <head>Danksagung</head>
                <p style="text-align: left; ">Das Projekt wird von der Deutschen Forschungsgemeinschaft (DFG) unter der Projektnummer 510048106 gefördert.</p>
            </div>
        </body>
        <back>
            <div type="bibliogr">
                <listBibl>
                    <head>Bibliographie</head>
                    <bibl style="text-align: left; ">
                        <hi rend="bold">Crawford, Kate und Trevor Paglen.</hi> 2021. „Correction to: Excavating AI. The Politics of Images in Machine Learning Training Sets.“ 
                        <hi rend="italic">AI &amp; Society</hi> 36.4: 1399 10.1007/s00146-021-01301-1.
                    </bibl>
                    <bibl style="text-align: left; ">
                        <hi rend="bold">Garcia, Noa und George Vogiatzis.</hi> 2018. „How to Read Paintings. Semantic Art Understanding with Multi-modal Retrieval.“ In 
                        <hi rend="italic">Computer Vision – ECCV 2018 Workshops. Lecture Notes in Computer Science</hi> 11130: 676–691 10.1007/978-3-030-11012-3_52.
                    </bibl>
                    <bibl style="text-align: left; ">
                        <hi rend="bold">Guidotti, Riccardo, Anna Monreale, Salvatore Ruggieri, Franco Turini, Fosca Giannotti und Dino Pedreschi.</hi> 2019. „A Survey of Methods for Explaining Black Box Models.“ In 
                        <hi rend="italic">ACM Computing Surveys</hi> 51.5: 93:1–93:42 10.1145/3236009.
                    </bibl>
                    <bibl style="text-align: left; ">
                        <hi rend="bold">Hensel, Thomas.</hi> 2010. „Aby Warburg und die‚Verschmelzende Vergleichsform‘.“ In 
                        <hi rend="italic">Vergleichendes Sehen</hi>, hg. von Lena Bader, Martin Gaier und Falk Wolf, 468–489. München: Fink.
                    </bibl>
                    <bibl style="text-align: left; ">
                        <hi rend="bold">Li, Junnan, Dongxu Li, Silvio Savarese und Steven C. H. Hoi</hi>2023. „BLIP-2: Bootstrapping Language-image Pre-training with Frozen Image Encoders and Large Language Models.“ In 
                        <hi rend="italic">International Conference on Machine Learning. ICML 2023</hi>, 19730–19742. https://proceedings.mlr.press/v202/li23q.html (zugegriffen: 3. Dezember 2023).
                    </bibl>
                    <bibl style="text-align: left; ">
                        <hi rend="bold">Liu, Haotian, Chunyuan Li, Qingyang Wu und Yong Jae Lee.</hi> 2023. 
                        <hi rend="italic">Visual Instruction Tuning. arXiv:2304.08485</hi>.
                    </bibl>
                    <bibl style="text-align: left; ">
                        <hi rend="bold">Offert, Fabian und Peter Bell.</hi> 2023. „imgs.ai. A Deep Visual Search Engine for Digital Art History.“ In 
                        <hi rend="italic">DH 2023. Digital Humanities 2023. Conference Abstracts</hi>, 141–142 10.5281/zenodo.7961822.
                    </bibl>
                    <bibl style="text-align: left; ">
                        <hi rend="bold">Offert, Fabian und Peter Bell.</hi> „Perceptual Bias and Technical Metapictures. Critical Machine Vision as a Humanities Challenge.“ 
                        <hi rend="italic" xml:space="preserve">AI &amp; Society </hi>36.4: 1133–1144 10.1007/s00146-020-01058-z.
                    </bibl>
                    <bibl style="text-align: left; ">
                        <hi rend="bold">OpenAI.</hi> 2023. 
                        <hi rend="italic">GPT-4 Technical Report</hi>. arXiv:2303.08774.
                    </bibl>
                    <bibl style="text-align: left; ">
                        <hi rend="bold">Pasquinelli, Matteo und Vladan Joler.</hi> 2021. „The Nooscope Manifested. AI as Instrument of Knowledge Extractivism.“ 
                        <hi rend="italic">AI &amp; Society</hi> 36.4: 1263–1280 10.1007/s00146-020-01097-6.
                    </bibl>
                    <bibl style="text-align: left; ">
                        <hi rend="bold">Radford, Alec, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger und Ilya Sutskever.</hi> 2021. „Learning Transferable Visual Models From Natural Language Supervision.“ In 
                        <hi rend="italic">Proceedings of the 38th International Conference on Machine Learning. ICML 2021</hi>, hg. von Marina Meila und Tong Zhang, 8748–8763, 
                        <ref target="http://proceedings.mlr.press/v139/radford21a.html">
                            <hi rend="color(1155CC)">http://proceedings.mlr.press/v139/radford21a.html</hi>
                        </ref> (zugegriffen: 19. Juli 2023).
                    </bibl>
                    <bibl style="text-align: left; ">
                        <hi rend="bold">Schneider, Stefanie, Matthias Springstein, Javad Rahnama, Hubertus Hohle, Ralph Ewerth und Eyke Hüllermeier.</hi> 2022. „iART. Eine Suchmaschine zur Unterstützung von bildorientierten Forschungsprozessen.“ In 
                        <hi rend="italic">8. Tagung des Verbands Digital Humanities im deutschsprachigen Raum. DHd 2022</hi>, hg. von Michaela Geierhos, 142–147 10.5281/zenodo.6304590.
                    </bibl>
                    <bibl style="text-align: left; ">
                        <hi rend="bold">Schrade, Torsten, und Linnaea Söhn.</hi> 2022. 
                        <hi rend="italic">Culture Portal 1.3. Knowledge Graph v.1.0 &amp; Repositorien Überblick</hi>. 
                        <ref target="https://nfdi4culture.de/de/nachrichten/culture-portal-13-knowledge-graph-v10-repository-overview.html">
                            <hi rend="color(1155CC)">https://nfdi4culture.de/de/nachrichten/culture-portal-13-knowledge-graph-v10-repository-overview.html</hi>
                        </ref> (zugegriffen: 19. Juli 2023).
                    </bibl>
                    <bibl style="text-align: left; ">
                        <hi rend="bold">Suhai, Mohammedl, Abhay Mittal, Behjat Siddiquie, Chris Broaddus, Jayan Eledath, Gérard G. Medioni und Leonid Sigal.</hi> 2021. „Energy-Based Learning for Scene Graph Generation.“ In 
                        <hi rend="italic">IEEE Conference on Computer Vision and Pattern Recognition. CVPR 2021</hi>, 13936–13945 10.1109/CVPR46437.2021.01372.
                    </bibl>
                    <bibl style="text-align: left; ">
                        <hi rend="bold">Wölfflin, Heinrich.</hi> 1915. 
                        <hi rend="italic">Kunstgeschichtliche Grundbegriffe. Das Problem der Stilentwickelung in der neueren Kunst.</hi> München: F. Bruckmann.
                    </bibl>
                </listBibl>
            </div>
        </back>
    </text>
</TEI>
